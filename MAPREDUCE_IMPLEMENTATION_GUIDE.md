# MapReduce Implementation Guide for Long-Form Audio Processing

**Status:** Design Document for Post-V1.0 Feature  
**Purpose:** Enable processing of 16+ hour audio files by implementing distributed summarization  
**Target:** Qwen3-4B-Instruct with 16K safe context (32K full)  
**Date:** December 27, 2025  
**Version:** 2.0 (Fact-Checked & Corrected)

---

## ðŸ”„ Version 2.0 Updates (Fact-Checked & Vietnamese-Optimized)

**Critical Corrections:**
- âœ… **Use existing tokenizer infrastructure** - Reuse `LLMProcessor._ensure_tokenizer()` and `calculate_dynamic_max_tokens()` from codebase
- âœ… **Research-backed overlap** - Increased to 2000 tokens (16.7%, within 10-20% best practice range)
- âœ… **Vietnamese sentence boundaries** - Use underthesea (most popular Vietnamese NLP toolkit) with regex fallback
- âœ… **RQ parallelization** - Use RQ queue instead of vLLM batch API (chat completions don't support batch)
- âœ… **Keep vLLM pipeline unchanged** - Do NOT modify existing vLLM infrastructure (easily breaks)
- âœ… **Validated against 2024-2025 research** - Google Cloud, Galileo AI, Pinecone, arxiv.org sources

**See [MAPREDUCE_FACT_CHECK.md](MAPREDUCE_FACT_CHECK.md) for detailed technical corrections.**

---

## Executive Summary

This document provides a complete design and implementation guide for adding MapReduce-based summarization to MAIE. It enables processing of extremely long transcripts (16+ hours of audio) that exceed the LLM's context window by breaking them into manageable chunks, processing each independently, and then recursively combining results.

**Key Outcomes:**
- âœ… Support arbitrary-length audio transcripts (tested up to 57,600+ tokens for 16 hours)
- âœ… Maintain semantic coherence across chunk boundaries
- âœ… Minimize quality degradation from chunking
- âœ… Minimize LLM inference cost
- âœ… Maintain compatibility with existing V1.0 API

---

## 1. Problem Analysis

### 1.1 Context Window Constraints

| Component | Value |
|-----------|-------|
| Qwen3-4B-Instruct Full Context | 32,768 tokens |
| Safe Default Context (Conservative) | 16,384 tokens |
| System Prompt + Schema | ~500-800 tokens |
| Available for Input | ~15,584 tokens |
| Safety Margin (20%) | 12,467 tokens |
| Practical Safe Limit | **~12,000 tokens** |

### 1.2 Audio to Token Conversion

```
Audio Duration â†’ Words â†’ Tokens
16 hours      = 57,600 sec Ã— (avg 2-3 words/sec) 
              = 115,200 - 172,800 words
              = 57,600 - 86,400 tokens (1 token â‰ˆ 0.75 words)
```

**Conclusion:** 16-hour audio generates 57,600-86,400 tokens, **4.8-7.2x the safe limit (16K)** or **1.8-2.9x even at full 32K window**.

### 1.3 V1.0 Limitation

Current implementation in [pipeline.py](src/worker/pipeline.py#L337):
```python
# Check Context Length: Count tokens in transcript and compare to LLM_CONTEXT_LENGTH
# Determine Processing Strategy (Task-Dependent Thresholds):
#   - Summarization: Threshold at 70% of context limit
#   - If above threshold: Use appropriate chunking strategy (MapReduce for summarization)
#   - If below threshold: Use direct processing
```

**Status:** Chunking logic exists in design but is **not implemented**. Current code assumes transcripts fit in context.

---

## 2. MapReduce Algorithm Design

### 2.1 High-Level Architecture

```
                    Long Transcript (57,600 tokens)
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   CHUNK SPLITTER        â”‚
                    â”‚  (overlapping windows)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚               â”‚               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚ CHUNK 1  â”‚    â”‚ CHUNK 2   â”‚   â”‚ CHUNK N   â”‚
         â”‚ 20K tokensâ”‚    â”‚ 20K tokensâ”‚   â”‚ 20K tokensâ”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                 â”‚               â”‚               â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
         â”‚         MAP PHASE (Parallel)               â”‚
         â”‚  LLM: Summarize each chunk independently  â”‚
         â”‚  Output: Mini-summaries (2-5K tokens each)â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   INTERMEDIATE LAYER     â”‚
         â”‚  Combine N mini-summariesâ”‚
         â”‚  (~10-25K tokens total)  â”‚
         â”‚  May need recursive map  â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    REDUCE PHASE          â”‚
         â”‚  LLM: Final summary      â”‚
         â”‚  Output: Final JSON      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Algorithm Steps

#### Phase 1: Chunk Splitting (Deterministic)

```python
def split_into_chunks(transcript: str, max_tokens: int = 12000, 
                     overlap_tokens: int = 2000) -> List[Tuple[str, int]]:
    """
    Split transcript into overlapping chunks for better continuity.
    
    Args:
        transcript: Full transcript text
        max_tokens: Target tokens per chunk (soft limit, respects sentence boundaries)
        overlap_tokens: Overlap between chunks (16.7%, research-backed 10-20% range)
    
    Returns:
        List of (chunk_text, token_count) tuples
    
    Algorithm:
        1. Tokenize entire transcript
        2. For each chunk_start position:
           a. Move forward by (max_tokens - overlap_tokens)
           b. Ensure boundary is at sentence/segment end (preserve meaning)
           c. Extract chunk [boundary - overlap ... boundary + max_tokens]
           d. Detokenize back to text
        3. Continue until transcript exhausted
    """
```

**Design Rationale:**
- **Conservative chunk size (12K tokens):** Leaves room for system prompt + schema (500-800 tokens) + 20% safety margin
- **Overlapping windows (2K tokens, 16.7%):** Research best practice (10-20% range) preserves context at boundaries and prevents topic splits
- **Sentence boundaries:** Research-critical feature; prevents splitting mid-sentence using spaCy/NLTK
- **Soft limits:** Respect text structure over exact token counts
- **Token counting:** Use fast tokenizer from Qwen3 model

#### Phase 2: Map Phase (Parallel Processing)

```python
def map_summarize_chunk(chunk: str, template_id: str, 
                        map_depth: int = 0) -> Dict[str, Any]:
    """
    LLM summarization of a single chunk.
    
    Args:
        chunk: Chunk text (â‰¤20K tokens)
        template_id: Summary template (e.g., "meeting_notes_v1")
        map_depth: Recursion depth (0=first level, 1+=recursive)
    
    Returns:
        {
            "summary": <structured JSON from template>,
            "chunk_index": <position in original transcript>,
            "token_count": <chunk token count>,
            "depth": <recursion level>,
            "model_info": <version info>
        }
    
    Execution:
        1. Call existing LLMProcessor.generate_summary() with chunk
        2. Validate JSON schema (same as V1.0)
        3. Store depth for later analysis
        4. Handle failures â†’ mark for retry or escalate
    """
```

**Parallelization Strategy:**
- Dispatch all chunks as separate Redis queue jobs (RQ supports this)
- Collect results with timeout (configurable, default 5 minutes per chunk)
- Alternative: Direct vLLM batch inference if server mode is used

#### Phase 3: Intermediate Layer

```python
def combine_chunk_summaries(chunk_summaries: List[Dict], 
                           template_id: str) -> Dict[str, Any]:
    """
    Combine N chunk summaries into intermediate representation.
    
    Decision Logic:
        - If total tokens â‰¤ 20K tokens: Go directly to REDUCE
        - If total tokens > 20K tokens: Recursive MAP with grouped chunks
    
    Grouping Strategy (for recursive case):
        1. Count tokens in each chunk summary
        2. Group summaries into sub-batches (target 16K tokens each)
        3. Create "meta-summaries" for each group by:
           - Concatenating chunk summaries
           - Adding context about order/boundaries
           - Running MAP again recursively
        4. Return grouped intermediate summaries
    
    Example:
        Input:  [S1, S2, S3, S4, S5] â†’ 24K tokens total
        Group:  [S1, S2] (8K) â†’ MetaS1, [S3, S4, S5] (14K) â†’ MetaS2
        Output: [MetaS1, MetaS2] â†’ 6K tokens â†’ Ready for REDUCE
    """
```

**Token Estimation:**
- First-level summaries typically compress to 10-20% of original
- E.g., 5 Ã— 20K chunks (100K tokens) â†’ 5 summaries (10-20K tokens)
- This usually fits in context for direct REDUCE

#### Phase 4: Reduce Phase

```python
def reduce_final_summary(intermediate_summaries: List[Dict], 
                         template_id: str,
                         original_transcript: str = None) -> Dict[str, Any]:
    """
    Combine intermediate summaries into final structured output.
    
    Args:
        intermediate_summaries: List of chunk/meta-summaries
        template_id: Final summary template
        original_transcript: Full transcript (for hallucination checking)
    
    Process:
        1. Concatenate all intermediate summaries
        2. Add instruction: "Synthesize the following chunk summaries 
           into a single coherent final summary"
        3. Call LLMProcessor.generate_summary() with concatenated input
        4. Validate final output against template schema
        5. Apply hallucination guardrails (entities must be in original text)
        6. Return final result
    
    Output:
        {
            "summary": <final structured JSON>,
            "processing_method": "mapreduce",
            "map_depth": <max recursion depth used>,
            "chunk_count": <N chunks processed>,
            "intermediate_summaries": [... optional for debugging],
            "versions": {...},
            "metrics": {...}
        }
    """
```

---

## 3. Implementation Architecture

### 3.1 Code Organization

```
src/
â”œâ”€â”€ processors/
â”‚   â””â”€â”€ llm/
â”‚       â”œâ”€â”€ processor.py (EXISTING - add new methods)
â”‚       â”œâ”€â”€ chunking.py (NEW)
â”‚       â”‚   â”œâ”€â”€ TokenCounter
â”‚       â”‚   â”œâ”€â”€ ChunkSplitter
â”‚       â”‚   â””â”€â”€ ChunkCombiner
â”‚       â””â”€â”€ mapreduce.py (NEW)
â”‚           â”œâ”€â”€ MapReduceOrchestrator
â”‚           â”œâ”€â”€ MapPhaseExecutor
â”‚           â””â”€â”€ ReducePhaseExecutor
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ token_counter.py (NEW - use Qwen3 tokenizer)
â”‚   â””â”€â”€ transcript_cleaner.py (NEW - clean chunks for better LLM processing)
â”‚
â””â”€â”€ worker/
    â”œâ”€â”€ pipeline.py (EXISTING - add context check + dispatch logic)
    â””â”€â”€ tasks.py (NEW - RQ task definitions for parallel map execution)
```

### 3.2 New Classes

#### 3.2.1 TokenCounter (USE EXISTING INFRASTRUCTURE)

**âš ï¸ IMPORTANT:** DO NOT create a new `TokenCounter` class. Use existing infrastructure:

**From `src/processors/llm/processor.py` (lines 447-492):**
```python
# EXISTING: LLMProcessor._ensure_tokenizer() method
# Preference order: vLLM tokenizer â†’ HuggingFace AutoTokenizer
# Already handles lazy initialization to avoid vLLM V1 blocking issues
```

**From `src/processors/llm/config.py` (lines 224-287):**
```python
# EXISTING: calculate_dynamic_max_tokens() function
# Already implements token counting with proper fallback:
def calculate_dynamic_max_tokens(input_text, tokenizer, task, max_model_len, user_override):
    try:
        # Prefer excluding special tokens for accurate input counting
        input_tokens = len(tokenizer.encode(input_text, add_special_tokens=False))
    except TypeError:
        input_tokens = len(tokenizer.encode(input_text))
    # ... industry-standard compression ratios ...
```

**Integration Approach:**
```python
class TokenCounter:
    """Wrapper for existing tokenizer infrastructure."""
    
    SAFE_CONTEXT_TOKENS = 16384  # 16K safe default
    SAFE_INPUT_TOKENS = 12000    # Leaves margin for system prompt
    
    def __init__(self, llm_processor: LLMProcessor):
        """Use tokenizer from existing LLMProcessor."""
        self.llm_processor = llm_processor
        # Ensure tokenizer is initialized (lazy init)
        self.llm_processor._ensure_tokenizer(
            self.llm_processor.model_path or "Qwen/Qwen2.5-4B-Instruct"
        )
    
    def count_tokens(self, text: str) -> int:
        """Count tokens using existing infrastructure."""
        tokenizer = self.llm_processor.tokenizer
        if tokenizer is None:
            # Fallback: rough estimate (1 token â‰ˆ 4 chars)
            return len(text) // 4
        
        try:
            # Exclude special tokens for accurate input counting
            return len(tokenizer.encode(text, add_special_tokens=False))
        except TypeError:
            return len(tokenizer.encode(text))
    
    @property
    def safe_context(self) -> int:
        return self.SAFE_CONTEXT_TOKENS
    
    @property
    def safe_input_tokens(self) -> int:
        return self.SAFE_INPUT_TOKENS
```

#### 3.2.2 ChunkSplitter

```python
class ChunkSplitter:
    """
    Split Vietnamese text into overlapping chunks while respecting sentence boundaries.
    Uses underthesea for Vietnamese sentence segmentation.
    """
    
    def __init__(self, token_counter: TokenCounter, 
                 max_tokens: int = 12000, overlap_tokens: int = 2000):
        self.token_counter = token_counter
        self.max_tokens = max_tokens      # 12K for 16K safe context
        self.overlap_tokens = overlap_tokens  # 2K = 16.7% (research: 10-20%)
        
        # Use underthesea for Vietnamese sentence splitting
        # underthesea is the most popular Vietnamese NLP toolkit
        # https://github.com/undertheseanlp/underthesea
        try:
            from underthesea import sent_tokenize
            self.sentence_splitter = sent_tokenize
            self.splitter_name = "underthesea"
        except ImportError:
            # Fallback: Simple regex for Vietnamese punctuation
            import re
            def vietnamese_sent_split(text):
                return re.split(r'(?<=[.!?])\s+', text)
            self.sentence_splitter = vietnamese_sent_split
            self.splitter_name = "regex_fallback"
            logger.warning("underthesea not available, using regex fallback")
    
    def split(self, text: str) -> List[Dict[str, Any]]:
        """
        Split text into overlapping chunks.
        
        Returns:
            List of {
                'text': chunk_text,
                'tokens': token_count,
                'start_char': char offset in original,
                'end_char': char offset in original,
                'boundary_context': {
                    'prev_sentence': str,
                    'next_sentence': str
                }
            }
        """
        chunks = []
        tokens_total = self.token_counter.count_tokens(text)
        
        # Get sentences using underthesea (Vietnamese-specific)
        sentences = self.sentence_splitter(text)
        logger.debug(f"Split into {len(sentences)} sentences using {self.splitter_name}")
        
        # Build chunks respecting sentence boundaries
        current_chunk_sentences = []
        current_tokens = 0
        start_idx = 0
        
        for sent_idx, sentence in enumerate(sentences):
            sent_tokens = self.token_counter.count_tokens(sentence)
            
            if current_tokens + sent_tokens > self.max_tokens and current_chunk_sentences:
                # Save chunk
                chunk_text = " ".join(current_chunk_sentences)
                chunks.append({
                    'text': chunk_text,
                    'tokens': current_tokens,
                    'index': len(chunks),
                    'sentence_range': (start_idx, sent_idx)
                })
                
                # Start new chunk with overlap
                overlap_sentences = [s for s in current_chunk_sentences 
                                    if self.token_counter.count_tokens(s) <= self.overlap_tokens]
                current_chunk_sentences = overlap_sentences + [sentence]
                current_tokens = sum(self.token_counter.count_tokens(s) 
                                   for s in current_chunk_sentences)
                start_idx = max(0, sent_idx - len(overlap_sentences))
            else:
                current_chunk_sentences.append(sentence)
                current_tokens += sent_tokens
        
        # Final chunk
        if current_chunk_sentences:
            chunks.append({
                'text': " ".join(current_chunk_sentences),
                'tokens': current_tokens,
                'index': len(chunks),
                'sentence_range': (start_idx, len(sentences))
            })
        
        return chunks
```

#### 3.2.3 MapReduceOrchestrator

```python
class MapReduceOrchestrator:
    """
    Orchestrates the full MapReduce pipeline for transcript summarization.
    Uses existing tokenizer infrastructure from LLMProcessor.
    """
    
    def __init__(self, llm_processor: LLMProcessor,
                 redis_conn: Redis,
                 queue: Queue = None):
        self.llm_processor = llm_processor
        # Use existing tokenizer infrastructure (no separate TokenCounter needed)
        self.token_counter = TokenCounter(llm_processor)
        self.redis_conn = redis_conn
        self.queue = queue or Queue(connection=redis_conn)
        self.chunk_splitter = ChunkSplitter(self.token_counter)
    
    def process(self, transcript: str, template_id: str, 
                task_id: str = None) -> Dict[str, Any]:
        """
        Execute full MapReduce pipeline.
        
        Decision Tree:
            1. Count tokens in transcript
            2. If â‰¤ 20K tokens: Use direct LLM.generate_summary()
            3. If > 20K tokens: 
               a. Split into chunks
               b. Dispatch MAP phase
               c. Collect intermediate summaries
               d. Check if recursive MAP needed
               e. Execute REDUCE phase
               f. Return final result
        """
        
        transcript_tokens = self.token_counter.count_tokens(transcript)
        logger.info(f"Transcript analysis: {transcript_tokens} tokens")
        
        # Direct processing if small enough (use conservative 16K limit)
        safe_context_limit = 12000  # Conservative: 16K / 1.33 safety factor
        if transcript_tokens <= safe_context_limit:
            logger.info("Transcript fits in context, using direct LLM processing")
            return self.llm_processor.generate_summary(
                transcript, template_id
            )
        
        logger.info("Transcript exceeds context, using MapReduce")
        
        # MapReduce execution
        try:
            # Phase 1: Chunking
            chunks = self.chunk_splitter.split(transcript)
            logger.info(f"Split transcript into {len(chunks)} chunks")
            
            # Phase 2: MAP
            chunk_summaries = self._execute_map_phase(
                chunks, template_id, task_id
            )
            logger.info(f"MAP phase complete: {len(chunk_summaries)} summaries")
            
            # Phase 3: Combine (with recursive decision)
            intermediate_summaries = self._combine_or_reduce(
                chunk_summaries, template_id, transcript
            )
            
            # Phase 4: REDUCE
            final_result = self._execute_reduce_phase(
                intermediate_summaries, template_id, transcript
            )
            
            # Annotate result
            final_result['processing_method'] = 'mapreduce'
            final_result['chunk_count'] = len(chunks)
            
            return final_result
            
        except Exception as e:
            logger.error(f"MapReduce failed: {e}")
            # Fallback: return best-effort result or error
            raise MapReduceError(str(e))
    
    def _execute_map_phase(self, chunks: List[Dict], 
                          template_id: str, task_id: str = None
                          ) -> List[Dict[str, Any]]:
        """
        Execute MAP phase (parallel chunk summarization).
        
        Strategy:
            Option A (RQ Queue - RECOMMENDED): Dispatch each chunk as separate RQ job
              - Natural parallelization via multiple workers
              - Fault-tolerant (failed chunks can be retried)
              - Existing infrastructure
            
            Option B (Sequential - FALLBACK): Process chunks one by one
              - Simple, reliable
              - Good for testing
            
            IMPORTANT: Do NOT modify existing vLLM pipeline (easily breaks)
            - Use existing LLMProcessor.generate_summary() method as-is
            - Leverage existing client infrastructure (LocalVllmClient/VllmServerClient)
            - MapReduce orchestration happens at task level, not vLLM level
            
            Note: vLLM batch API not available for chat completions (structured outputs)
        """
        
        # Use RQ queue for parallelization if enabled
        if settings.mapreduce.use_parallel_rq:
            return self._execute_map_parallel_rq(chunks, template_id, task_id)
        else:
            # Sequential execution (v1.0 compatible)
            return self._execute_map_sequential(chunks, template_id, task_id)
    
    def _execute_map_sequential(self, chunks: List[Dict], 
                               template_id: str,
                               task_id: str = None) -> List[Dict[str, Any]]:
        """Summarize chunks sequentially."""
        summaries = []
        for i, chunk in enumerate(chunks):
            logger.info(f"Summarizing chunk {i+1}/{len(chunks)}")
            try:
                result = self.llm_processor.generate_summary(
                    chunk['text'], template_id
                )
                result['chunk_index'] = i
                summaries.append(result)
            except Exception as e:
                logger.error(f"Failed to summarize chunk {i}: {e}")
                # Include error marker for later handling
                summaries.append({
                    'error': str(e),
                    'chunk_index': i,
                    'summary': None
                })
        return summaries
    
    def _execute_map_parallel_rq(self, chunks: List[Dict], 
                                 template_id: str,
                                 task_id: str = None) -> List[Dict]:
        """Dispatch chunks as separate RQ jobs for parallel processing."""
        from rq import Queue
        
        job_ids = []
        for i, chunk in enumerate(chunks):
            job = self.queue.enqueue(
                'src.worker.mapreduce_tasks.summarize_chunk',
                args=(chunk['text'], template_id, self.llm_processor.model_path),
                timeout='10m',
                result_ttl=3600,
                meta={'chunk_index': i, 'parent_task_id': task_id}
            )
            job_ids.append((job.id, i))
        
        # Collect results with timeout
        summaries = []
        for job_id, chunk_idx in job_ids:
            try:
                job = self.queue.fetch_job(job_id)
                result = job.result  # Blocks until complete
                result['chunk_index'] = chunk_idx
                summaries.append(result)
            except Exception as e:
                logger.error(f"Failed to collect result for chunk {chunk_idx}: {e}")
                summaries.append({
                    'error': str(e),
                    'chunk_index': chunk_idx,
                    'summary': None
                })
        
        return summaries
    
    def _combine_or_reduce(self, summaries: List[Dict], 
                          template_id: str,
                          original_transcript: str) -> List[Dict]:
        """
        Decide whether to do recursive MAP or go straight to REDUCE.
        
        Returns:
            Summaries ready for REDUCE phase (either original or recursively processed)
        """
        
        # Estimate total token count of all summaries
        summary_texts = [
            json.dumps(s['summary']) if s.get('summary') else ""
            for s in summaries
        ]
        total_tokens = sum(self.token_counter.count_tokens(t) for t in summary_texts)
        
        logger.info(f"Intermediate summaries: {total_tokens} tokens")
        
        # If summaries fit in safe context, ready for REDUCE
        safe_context_limit = 12000
        if total_tokens <= safe_context_limit:
            logger.info("Intermediate summaries fit in context, proceeding to REDUCE")
            return summaries
        
        # Need recursive MAP
        logger.info("Intermediate summaries too large, executing recursive MAP")
        
        # Create meta-summaries by grouping
        meta_summaries = self._group_and_summarize(
            summaries, template_id
        )
        
        # Recurse if still too large
        meta_total_tokens = sum(
            self.token_counter.count_tokens(json.dumps(s.get('summary', {})))
            for s in meta_summaries
        )
        
        if meta_total_tokens > self.token_counter.max_context * 0.65:
            return self._combine_or_reduce(
                meta_summaries, template_id, original_transcript
            )
        
        return meta_summaries
    
    def _group_and_summarize(self, summaries: List[Dict], 
                            template_id: str) -> List[Dict]:
        """Group summaries and create meta-summaries."""
        target_tokens = 10000  # Group to stay well below 12K safe limit
        groups = []
        current_group = []
        current_tokens = 0
        
        for summary in summaries:
            summary_tokens = self.token_counter.count_tokens(
                json.dumps(summary.get('summary', {}))
            )
            
            if current_tokens + summary_tokens > target_tokens and current_group:
                # Summarize group
                group_text = "\n---\n".join(
                    json.dumps(s.get('summary', {})) for s in current_group
                )
                groups.append(self.llm_processor.generate_summary(
                    f"Combine these summaries:\n{group_text}", template_id
                ))
                current_group = [summary]
                current_tokens = summary_tokens
            else:
                current_group.append(summary)
                current_tokens += summary_tokens
        
        # Final group
        if current_group:
            group_text = "\n---\n".join(
                json.dumps(s.get('summary', {})) for s in current_group
            )
            groups.append(self.llm_processor.generate_summary(
                f"Combine these summaries:\n{group_text}", template_id
            ))
        
        return groups
    
    def _execute_reduce_phase(self, summaries: List[Dict], 
                             template_id: str,
                             original_transcript: str = None) -> Dict[str, Any]:
        """
        Execute REDUCE phase (final synthesis).
        
        Prompt structure:
            System: "You are synthesizing multiple chunk summaries..."
            User: "Combine these summaries: <formatted summaries>"
        """
        
        formatted_summaries = "\n\n---\n\n".join(
            json.dumps(s.get('summary', {}), indent=2)
            for s in summaries if s.get('summary')
        )
        
        reduce_prompt = f"""
Synthesize the following chunk summaries into a single coherent final summary.
Ensure:
1. No contradictions between chunk summaries
2. All important entities/facts are preserved
3. Chronological or logical order is maintained
4. Output is valid JSON matching the template schema

CHUNK SUMMARIES:
{formatted_summaries}
"""
        
        # Execute final LLM call
        final_result = self.llm_processor.generate_summary(
            reduce_prompt, template_id
        )
        
        # Apply hallucination guardrails if original transcript available
        if original_transcript and final_result.get('summary'):
            final_result['summary'] = self.llm_processor._postprocess_summary(
                template_id, original_transcript, final_result['summary']
            )
        
        return final_result
```

---

## 4. Integration with Existing Pipeline

### 4.1 Modify Worker Pipeline

**File:** [pipeline.py](src/worker/pipeline.py#L1350-L1390)

```python
def process_audio_task(task_params: Dict[str, Any]) -> Dict[str, Any]:
    # ... existing ASR code ...
    
    # Summary
    if "summary" in features:
        if not template_id:
            raise LLMProcessingError("template_id required for summary")
        
        try:
            # NEW: Check context length before processing
            transcript_tokens = llm_model.token_counter.count_tokens(clean_transcript)
            context_limit = settings.llm_sum.max_model_len or 32768
            safe_threshold = context_limit * 0.70  # 70% for summarization
            
            if transcript_tokens > safe_threshold:
                logger.warning(
                    f"Transcript exceeds safe context ({transcript_tokens} > {safe_threshold} tokens), "
                    f"using MapReduce"
                )
                # NEW: Use MapReduce (tokenizer handled internally)
                mapreduce = MapReduceOrchestrator(
                    llm_model,
                    redis_conn,
                    queue
                )
                summary_result = mapreduce.process(
                    clean_transcript, template_id, job_id
                )
            else:
                # Direct processing (existing code)
                summary_result = llm_model.generate_summary(
                    transcript=clean_transcript, template_id=template_id
                )
            
            if summary_result.get("summary"):
                structured_summary = summary_result["summary"]
                logger.info(f"Summary generated, method={summary_result.get('processing_method', 'direct')}")
            else:
                raise LLMProcessingError(f"Summary generation failed: {summary_result.get('error')}")
        
        except LLMProcessingError:
            raise
```

### 4.2 Extend LLMProcessor

**File:** [processor.py](src/processors/llm/processor.py#L55-L100)

```python
class LLMProcessor(LLMBackend):
    def __init__(self, model_path: Optional[str] = None, **kwargs):
        # ... existing code ...
        
        # NEW: Initialize token counter
        try:
            self.token_counter = TokenCounter(model_path or settings.llm_enhance.model)
        except Exception as e:
            logger.warning(f"Failed to initialize token counter: {e}")
            self.token_counter = None
    
    # NEW: Add property for safe context threshold
    @property
    def safe_context_tokens(self) -> int:
        """Get safe token limit for direct processing (16K conservative default)."""
        if self.token_counter:
            return self.token_counter.safe_input_tokens
        return 12000
```

---

## 5. Configuration & Settings

### 5.1 Environment Variables

Add to `.env.template`:

```bash
# MapReduce Configuration (Vietnamese-Optimized)
MAPREDUCE_ENABLED=true                       # Enable MapReduce for long transcripts
MAPREDUCE_CHUNK_SIZE_TOKENS=12000            # 12K chunks for 16K safe context
MAPREDUCE_CHUNK_OVERLAP_TOKENS=2000          # 16.7% overlap (research: 10-20%)
MAPREDUCE_MAX_RECURSION_DEPTH=3              # Max recursive MAP iterations
MAPREDUCE_TIMEOUT_SECONDS=3600               # Total timeout (1 hour)
MAPREDUCE_PARALLEL_WORKERS=4                 # RQ workers for parallel MAP phase
MAPREDUCE_USE_PARALLEL_RQ=true               # Use RQ queue for parallelization
MAPREDUCE_SENTENCE_SPLITTER=underthesea      # underthesea (Vietnamese) or regex fallback
MAPREDUCE_COMPRESSION_RATIO_THRESHOLD=0.15   # Alert if compression <15%
MAPREDUCE_SAFE_CONTEXT_TOKENS=16384          # Conservative safe context (16K)
```

### 5.2 Settings Model

**File:** [model.py](src/config/model.py#L600-L650)

```python
class MapReduceSettings(BaseModel):
    """MapReduce configuration for long-form processing (16K safe context default)."""
    
    enabled: bool = Field(
        default=True,
        description="Enable MapReduce for transcripts exceeding 16K safe context"
    )
    chunk_size_tokens: int = Field(
        default=12000,
        ge=5000,
        le=15000,
        description="Target tokens per chunk (conservative: leaves room for system prompt)"
    )
    chunk_overlap_tokens: int = Field(
        default=2000,
        ge=500,
        le=3000,
        description="Overlap tokens between chunks (research: 16.7%, within 10-20% range)"
    )
    max_recursion_depth: int = Field(
        default=3,
        ge=1,
        le=10,
        description="Maximum recursive MAP iterations"
    )
    timeout_seconds: int = Field(
        default=3600,
        ge=300,
        description="Total timeout for MapReduce pipeline"
    )
    parallel_workers: int = Field(
        default=4,
        ge=1,
        le=16,
        description="Parallel workers for MAP phase"
    )
    use_parallel_rq: bool = Field(
        default=True,
        description="Use RQ queue for parallel MAP execution (recommended)"
    )
    sentence_splitter: str = Field(
        default="underthesea",
        description="Sentence splitter: 'underthesea' (Vietnamese) or 'regex' fallback"
    )
    compression_ratio_threshold: float = Field(
        default=0.15,
        ge=0.05,
        le=0.50,
        description="Alert if compression ratio falls below this threshold"
    )

class AppSettings(BaseSettings):
    # ... existing settings ...
    mapreduce: MapReduceSettings = Field(default_factory=MapReduceSettings)
```

---

## 6. Error Handling & Robustness

### 6.1 Failure Scenarios

| Scenario | Handling |
|----------|----------|
| **Single chunk fails** | Mark with error, include in results, try remaining chunks |
| **MAP phase timeout** | Fail gracefully, partial results available |
| **LLM returns invalid JSON** | Retry with lower temperature (existing retry logic) |
| **Recursive depth exceeded** | Truncate, process available chunks, return partial result |
| **Out of memory** | Reduce chunk size, re-execute |

### 6.2 Implementation

```python
class MapReduceError(Exception):
    """Base exception for MapReduce failures."""
    pass

class MapPhaseError(MapReduceError):
    """MAP phase failed."""
    pass

class ReducePhaseError(MapReduceError):
    """REDUCE phase failed."""
    pass

# In orchestrator:
def process(self, transcript: str, template_id: str) -> Dict[str, Any]:
    try:
        # ... normal flow ...
    except MapPhaseError as e:
        logger.error(f"MAP phase failure: {e}")
        # Try fallback: reduce chunk size and retry
        if not self._retry_attempted:
            return self.process_with_smaller_chunks(transcript, template_id)
        else:
            raise
    except ReducePhaseError as e:
        logger.error(f"REDUCE phase failure: {e}")
        # Try fallback: return best-effort result from intermediate summaries
        return self._best_effort_result(e)
```

---

## 7. Testing Strategy

### 7.1 Unit Tests

```python
# tests/processors/llm/test_mapreduce.py

def test_chunk_splitter():
    """Test ChunkSplitter with various transcript lengths."""
    splitter = ChunkSplitter(token_counter, max_tokens=20000)
    
    # 10-hour audio (~28,800 tokens)
    transcript_10h = generate_mock_transcript(28800)
    chunks = splitter.split(transcript_10h)
    
    assert len(chunks) >= 2
    assert all(c['tokens'] <= 20500 for c in chunks)  # Soft limit with margin
    assert chunks[0]['tokens'] > 0

def test_mapreduce_direct_processing():
    """Small transcript should use direct path."""
    small_transcript = generate_mock_transcript(10000)
    orchestrator = MapReduceOrchestrator(llm_processor, token_counter, redis)
    
    result = orchestrator.process(small_transcript, "meeting_notes_v1")
    
    assert result.get('processing_method') != 'mapreduce'

def test_mapreduce_with_real_transcript():
    """Test with realistic 16-hour audio (~60K tokens)."""
    long_transcript = generate_mock_transcript(60000)
    orchestrator = MapReduceOrchestrator(llm_processor, token_counter, redis)
    
    result = orchestrator.process(long_transcript, "meeting_notes_v1")
    
    assert result['processing_method'] == 'mapreduce'
    assert result['chunk_count'] >= 3
    assert result['summary'] is not None
    assert result['summary'].get('meeting_date') is not None  # Schema validation
```

### 7.2 Integration Tests

```python
def test_mapreduce_end_to_end_with_mock_llm():
    """End-to-end test with mock LLM responses."""
    # Mock LLMProcessor to return deterministic results
    with mock_llm_processor():
        long_transcript = load_sample_transcript("16_hours.txt")
        task_params = {
            'audio_path': '/tmp/mock_16h_audio.wav',
            'features': ['clean_transcript', 'summary'],
            'template_id': 'meeting_notes_v1',
            'enable_diarization': False
        }
        
        result = process_audio_task(task_params)
        
        assert result['status'] == 'complete'
        assert result['summary']['processing_method'] == 'mapreduce'
        assert len(result['summary']['chunk_count']) >= 3

def test_mapreduce_failure_recovery():
    """Test graceful degradation on failures."""
    long_transcript = generate_mock_transcript(60000)
    
    # Mock one chunk failure
    with mock_llm_failure(chunk_index=1):
        orchestrator = MapReduceOrchestrator(llm_processor, token_counter, redis)
        result = orchestrator.process(long_transcript, "meeting_notes_v1")
        
        # Should still succeed with remaining chunks
        assert result['summary'] is not None
        assert 'partial_failure' in result.get('processing_notes', '')
```

---

## 8. Performance Optimization

### 8.1 Token Counting Optimization

```python
class TokenCounter:
    _cache = {}  # LRU cache of text â†’ token count
    _max_cache_size = 10000
    
    def count_tokens(self, text: str) -> int:
        if text in self._cache:
            return self._cache[text]
        
        count = len(self.tokenizer.encode(text))
        
        # Simple LRU: evict oldest if cache full
        if len(self._cache) > self._max_cache_size:
            oldest = min(self._cache, key=lambda k: self._cache[k][1])
            del self._cache[oldest]
        
        self._cache[text] = (count, time.time())
        return count
```

### 8.2 Parallel Execution

```python
# For vLLM Server mode: use batch API
def _execute_map_batch(self, chunks: List[Dict], template_id: str):
    """Use vLLM batch/scheduling optimization."""
    from vllm import LLM
    
    # If using local LLM, batch sequences
    if self.llm_processor.model:
        from vllm import SamplingParams
        
        # Prepare all prompts
        prompts = [chunk['text'] for chunk in chunks]
        
        # Batch generate summaries
        sampling_params = SamplingParams(
            temperature=settings.llm_sum.temperature,
            max_tokens=settings.llm_sum.max_tokens,
        )
        
        outputs = self.llm_processor.model.generate(
            prompts, sampling_params
        )
        
        return [
            {'summary': parse_json(o.outputs[0].text), 'chunk_index': i}
            for i, o in enumerate(outputs)
        ]
```

---

## 9. Dependencies & Installation

### 9.1 Required Packages

**New dependency for Vietnamese sentence segmentation:**

```bash
# Add to pyproject.toml or requirements.txt
underthesea>=1.3.5  # Vietnamese NLP toolkit
```

**Installation:**
```bash
pip install underthesea
```

**API Documentation:**
```python
from underthesea import sent_tokenize

# Example usage
text = "Taylor cho biáº¿t lÃºc Ä‘áº§u cÃ´ cáº£m tháº¥y ngáº¡i. Amanda cÅ©ng thoáº£i mÃ¡i."
sentences = sent_tokenize(text)
# Returns: ["Taylor cho biáº¿t lÃºc Ä‘áº§u cÃ´ cáº£m tháº¥y ngáº¡i.", "Amanda cÅ©ng thoáº£i mÃ¡i."]
```

### 9.2 Fallback Behavior

If `underthesea` is not installed, system falls back to regex-based sentence splitting:
- Splits on Vietnamese sentence endings: `.` `!` `?`
- Less accurate but functional
- Logs warning: `"underthesea not available, using regex fallback"`

### 9.3 vLLM Pipeline - Do NOT Modify

**CRITICAL:** Keep existing vLLM infrastructure unchanged:
- Existing `LLMProcessor._load_model()` - DO NOT TOUCH
- Existing client infrastructure (`LocalVllmClient`, `VllmServerClient`) - DO NOT TOUCH  
- Existing `generate_summary()` method - USE AS-IS
- MapReduce sits **above** vLLM layer, not inside it

**Why:** vLLM pipeline is fragile and complex:
- Multi-process initialization
- Server vs local mode switching
- Quantization detection
- Tokenizer lazy loading
- Any changes risk breaking existing functionality

---

## 10. API Compatibility

### 10.1 No Breaking Changes

MapReduce is **completely invisible** to API users:

```python
# Client code (unchanged)
response = requests.post(
    'http://localhost:8000/v1/process',
    headers={'X-API-Key': 'your-key'},
    files={'file': open('16_hour_meeting.wav', 'rb')},
    data={'features': ['summary'], 'template_id': 'meeting_notes_v1'}
)

# Returns same schema as V1.0
print(response.json()['summary']['summary'])

# BUT: includes new field for transparency
print(response.json()['summary'].get('processing_method'))  # 'mapreduce'
```

### 10.2 Response Schema Extension

```json
{
  "task_id": "...",
  "status": "complete",
  "summary": {
    "processing_method": "mapreduce",  // NEW: "direct" or "mapreduce"
    "chunk_count": 4,                   // NEW: number of chunks processed
    "recursion_depth": 1,               // NEW: max recursion level
    "summary": {...},                   // EXISTING: structured summary
    "raw_output": "...",
    "tags": [...],
    "versions": {...},
    "metrics": {...}
  }
}
```

---

## 11. Rollout Plan (Vietnamese-Optimized)

### Phase 1: Foundation (Week 1-2)
- âœ… Install underthesea dependency (`pip install underthesea`)
- âœ… Implement TokenCounter wrapper (uses existing `_ensure_tokenizer()`)
- âœ… Implement ChunkSplitter with underthesea sentence segmentation
- âœ… Unit tests (90% coverage) - include Vietnamese text samples
- âœ… Code review - verify vLLM pipeline untouched

### Phase 2: MapReduce Core (Week 2-3)
- âœ… Implement MapReduceOrchestrator (uses existing `generate_summary()`)
- âœ… Integrate with pipeline.py (context check only)
- âœ… Integration tests with Vietnamese transcripts
- âœ… Performance profiling

### Phase 3: Optimization (Week 3-4)
- âœ… RQ parallel execution (NOT vLLM batch API)
- âœ… Recursive MAP/REDUCE validation
- âœ… Load testing (16+ hour Vietnamese audio)

### Phase 4: Production (Week 6)
- âœ… Feature flag (optional)
- âœ… Monitoring/logging
- âœ… Documentation
- âœ… Release as V1.1

---

## 11. Monitoring & Observability

### 11.1 Metrics to Track

```python
# In MapReduceOrchestrator:
metrics = {
    'mapreduce_invocations': Counter(),
    'chunk_count': Histogram(),
    'map_phase_duration_seconds': Histogram(),
    'reduce_phase_duration_seconds': Histogram(),
    'recursion_depth_used': Histogram(),
    'mapreduce_errors': Counter(labels=['stage', 'error_type']),
}
```

### 11.2 Logging

```python
logger.info(
    "MapReduce execution summary",
    transcript_tokens=transcript_tokens,
    chunk_count=len(chunks),
    recursion_depth=recursion_depth,
    total_duration_seconds=total_duration,
    map_phase_duration=map_duration,
    reduce_phase_duration=reduce_duration,
)
```

---

## 12. Future Enhancements

- **Adaptive chunk sizing:** Adjust chunk size based on content type
- **Parallel REDUCE:** Use multiple LLM instances for truly parallel final synthesis
- **Incremental summarization:** Stream results as chunks are processed
- **Hierarchical templates:** Different templates for chunk vs. final summary
- **Quality scoring:** Assess summary coherence across chunk boundaries

---

## References

### Research & Best Practices (2024-2025)

- **Google Cloud Blog:** "Long-document summarization with Workflows and Gemini models" - 64K character chunks recommended
- **Galileo AI:** "Master LLM Summarization Strategies" - Chunk overlap prevents topic splits
- **arxiv.org/html/2410.09342v1:** "LLMÃ—MapReduce: Simplified Long-Sequence Processing" - CoA workflow, LC-Boost patterns
- **Pinecone:** "Chunking Strategies for LLM Applications" - Contextual retrieval (Anthropic 2024)
- **Belitsoft & DevTo:** MapReduce pattern for long documents - chunk â†’ map â†’ reduce
- **Industry Standard:** 10-20% overlap ratio, 8K-16K token chunks for robust LLMs

### Existing MAIE Codebase

- [processor.py](src/processors/llm/processor.py#L447-492) - `_ensure_tokenizer()` implementation
- [config.py](src/processors/llm/config.py#L224-287) - `calculate_dynamic_max_tokens()` function
- [TDD.md](docs/TDD.md#L337) - Context length checking placeholder
- [pipeline.py](src/worker/pipeline.py#L1350) - process_audio_task() integration point
- [Brief and MVP.md](docs/Brief%20and%20MVP.md#L33) - V1.0 limitations

### Related Documentation

- [MAPREDUCE_FACT_CHECK.md](MAPREDUCE_FACT_CHECK.md) - Technical corrections and validation

---

## Appendix A: Code Templates

### A.1 Minimal Example

```python
# Minimal MapReduce implementation
from src.processors.llm.mapreduce import MapReduceOrchestrator

# Usage
orchestrator = MapReduceOrchestrator(llm_processor, token_counter, redis_conn)
result = orchestrator.process(
    transcript="[57,600 tokens of text...]",
    template_id="meeting_notes_v1"
)

# Result includes:
# {
#     "processing_method": "mapreduce",
#     "chunk_count": 4,
#     "summary": {...}  # Final structured JSON
# }
```

### A.2 Testing Mock

```python
# Mock LLM processor for testing
class MockLLMProcessor:
    def generate_summary(self, transcript, template_id):
        return {
            "summary": {
                "meeting_date": "2025-01-15",
                "participants": ["Alice", "Bob"],
                "key_points": ["Point 1", "Point 2"],
                "action_items": ["TODO 1"]
            },
            "raw_output": json.dumps({...}),
            "retry_count": 0
        }
```

---

**Document Version:** 1.0  
**Last Updated:** December 27, 2025  
**Status:** Ready for Implementation  
**Next Steps:** Assign to development, begin Phase 1
