# Docker Compose Configuration for Modular Audio Intelligence Engine (MAIE)
#
# This configuration orchestrates the three-tier architecture with GPU support
# for on-premises deployment as described in TDD Section 3.6.
#
# Services:
# - api: Litestar HTTP server (stateless)
# - worker: RQ worker with GPU access (stateless, sequential processing)
# - redis: Dual-DB Redis (queue + results with AOF persistence)
# - rq-dashboard: Queue monitoring
# - jaeger: Distributed tracing (optional)

version: "3.8"

services:
  # Redis - Job queue (DB 0) + Results store (DB 1)
  redis:
    image: redis:7-alpine
    container_name: maie-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --appendfsync everysec --databases 2
    volumes:
      - ./data/redis:/data # AOF persistence for results DB
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # API Server - Stateless HTTP endpoint
  api:
    build:
      context: .
      target: production
    container_name: maie-api
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - ./data/audio:/data/audio:rw # Audio uploads
      - ./templates:/app/templates:ro # Template schemas
      - ./assets/chat-templates:/app/assets/chat-templates:ro # Chat templates
    environment:
      - REDIS_URL=redis://redis:6379/0
      - AUDIO_DIR=/data/audio
      - TEMPLATES_DIR=/app/templates
      - CHAT_TEMPLATES_DIR=/app/assets/chat-templates
      - MAX_QUEUE_DEPTH=${MAX_QUEUE_DEPTH:-1000}
      - SECRET_API_KEY=${SECRET_API_KEY}
      - DEBUG=${DEBUG:-false}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - MAX_FILE_SIZE_MB=500
      - REDIS_RESULTS_DB=1
      - WORKER_NAME=maie-worker
      - JOB_TIMEOUT=600
      - RESULT_TTL=86400
      - WHISPER_MODEL_VARIANT=erax-wow-turbo
      - WHISPER_BEAM_SIZE=5
      - WHISPER_VAD_FILTER=True
      - WHISPER_COMPUTE_TYPE=int8_float16
      - LLM_MODEL_NAME=cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit
      - LLM_GPU_MEMORY_UTILIZATION=0.9
      - LLM_MAX_MODEL_LEN=32768
      - LLM_TEMPERATURE=0.3
      - LLM_TOP_P=0.9
      - LLM_TOP_K=20
      - LLM_MAX_TOKENS=4096
    command: ["python3", "-m", "src.api.main"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # GPU Worker - Sequential AI model processing
  worker:
    build:
      context: .
      target: production
    container_name: maie-worker
    restart: unless-stopped
    depends_on:
      redis:
        condition: service_healthy
    volumes:
      - ./data/audio:/data/audio:ro # Audio files (read-only)
      - ./data/models:/data/models:ro # AI models (read-only)
      - ./templates:/app/templates:ro # Template schemas
      - ./assets/chat-templates:/app/assets/chat-templates:ro # Chat templates
    environment:
      - REDIS_URL=redis://redis:6379/0
      - AUDIO_DIR=/data/audio
      - MODEL_DIR=/data/models
      - TEMPLATES_DIR=/app/templates
      - CHAT_TEMPLATES_DIR=/app/assets/chat-templates
      - WHISPER_MODEL_VARIANT=${WHISPER_MODEL_VARIANT:-erax-wow-turbo}
      - WHISPER_BEAM_SIZE=5
      - WHISPER_VAD_FILTER=True
      - WHISPER_VAD_MIN_SILENCE_MS=500
      - WHISPER_VAD_SPEECH_PAD_MS=400
      - WHISPER_DEVICE=cuda
      - WHISPER_COMPUTE_TYPE=float16
      - WHISPER_CPU_FALLBACK=true
      - WHISPER_CONDITION_ON_PREVIOUS_TEXT=true
      - WHISPER_LANGUAGE=
      - OMP_NUM_THREADS=4
      - LD_LIBRARY_PATH=/usr/local/lib/python3.11/site-packages/nvidia/cublas/lib:/usr/local/lib/python3.11/site-packages/nvidia/cudnn/lib
      - LLM_CONTEXT_LENGTH=${LLM_CONTEXT_LENGTH:-4096}
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-32768}
      - TEMPERATURE=${TEMPERATURE:-0.7}
      - MAX_TOKENS=${MAX_TOKENS:-2048}
      - TOP_P=${TOP_P:-0.9}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.9}
      - DEBUG=${DEBUG:-false}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - API_HOST=0.0.0.0
      - API_PORT=8000
      - MAX_FILE_SIZE_MB=500
      - REDIS_RESULTS_DB=1
      - WORKER_NAME=maie-worker
      - JOB_TIMEOUT=600
      - RESULT_TTL=86400
      - WHISPER_BEAM_SIZE=5
      - WHISPER_VAD_FILTER=True
      - WHISPER_COMPUTE_TYPE=int8_float16
      - LLM_MODEL_NAME=cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit
      - LLM_GPU_MEMORY_UTILIZATION=0.9
      - LLM_MAX_MODEL_LEN=32768
      - LLM_TEMPERATURE=0.3
      - LLM_TOP_P=0.9
      - LLM_TOP_K=20
      - LLM_MAX_TOKENS=4096
    command: ["python3", "-m", "src.worker.main"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]
    healthcheck:
      test:
        [
          "CMD",
          "python3",
          "-c",
          "import torch; print('GPU available:', torch.cuda.is_available())",
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # RQ Dashboard - Queue monitoring
  rq-dashboard:
    image: mdevilliers/rq-dashboard:latest
    container_name: maie-rq-dashboard
    restart: unless-stopped
    depends_on:
      - redis
    ports:
      - "9181:9181"
    environment:
      - RQ_DASHBOARD_REDIS_URL=redis://redis:6379/0
      - RQ_DASHBOARD_POLL_INTERVAL=2000
    command: ["rq-dashboard", "-H", "redis", "-p", "9181", "-P", "6379"]

  # Jaeger - Distributed tracing (optional)
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: maie-jaeger
    restart: unless-stopped
    ports:
      - "16686:16686" # UI
      - "4318:4318" # OTLP HTTP
      - "14268:14268" # Jaeger HTTP
    environment:
      - COLLECTOR_OTLP_ENABLED=true
    volumes:
      - ./data/jaeger:/tmp/jaeger

volumes:
  redis_data:
    driver: local
  audio_data:
    driver: local
  model_data:
    driver: local
  template_data:
    driver: local
  chat_templates_data:
    driver: local
