# ============================================================================
# MAIE (Modular Audio Intelligence Engine) - Configuration Template
# ============================================================================
# Copy this file to .env and fill in your values
# Version: 1.0.0

# ============================================================================
# Core Settings
# ============================================================================
PIPELINE_VERSION=1.0.0
ENVIRONMENT=development
DEBUG=false

# ============================================================================
# API Server Settings
# ============================================================================
API_HOST=0.0.0.0
API_PORT=8000
SECRET_API_KEY=CHANGE_ME_TO_A_SECURE_KEY
MAX_FILE_SIZE_MB=500

# ============================================================================
# Redis Settings
# ============================================================================
REDIS_URL=redis://localhost:6379/0
REDIS_RESULTS_DB=1
MAX_QUEUE_DEPTH=50

# ============================================================================
# ASR Settings - Whisper (Default Backend)
# ============================================================================
# Model Configuration
WHISPER_MODEL_PATH=data/models/era-x-wow-turbo-v1.1-ct2
WHISPER_MODEL_VARIANT=erax-wow-turbo
# Options: erax-wow-turbo, large-v3, turbo, distil-large-v3

# Decoding Parameters
WHISPER_BEAM_SIZE=5
WHISPER_VAD_FILTER=true
WHISPER_VAD_MIN_SILENCE_MS=500
WHISPER_VAD_SPEECH_PAD_MS=400
WHISPER_DEVICE=cuda
# Options: cuda, cpu, auto

WHISPER_COMPUTE_TYPE=float16
# Options: float16, int8, int8_float16

WHISPER_CPU_FALLBACK=false
WHISPER_CONDITION_ON_PREVIOUS_TEXT=true
# Note: Set to false for Distil-Whisper models

# Language Settings
WHISPER_LANGUAGE=auto
# Set explicitly to skip auto-detection (recommended for offline deployment and tests)
# Options: en, vi, etc. or leave empty for auto-detection (may cause issues in multi-threaded contexts)

WHISPER_CPU_THREADS=
# Leave empty for auto, or set to number of threads (e.g., 4)

# ============================================================================
# ASR Settings - ChunkFormer (Long-form Backend)
# ============================================================================
# Model Configuration
CHUNKFORMER_MODEL_PATH=data/models/chunkformer-ctc-large-vie
CHUNKFORMER_MODEL_VARIANT=khanhld/chunkformer-large-vie

# ChunkFormer Decoding Parameters (CORRECTED VALUES - V1.0)
CHUNKFORMER_CHUNK_SIZE=64
# Chunk size in FRAMES (not samples) - Default: 64

CHUNKFORMER_LEFT_CONTEXT_SIZE=128
# Left context window size in FRAMES - Default: 128

CHUNKFORMER_RIGHT_CONTEXT_SIZE=128
# Right context window size in FRAMES - Default: 128

CHUNKFORMER_TOTAL_BATCH_DURATION=14400
# Total batch duration in SECONDS (not ms) - Default: 14400 (4 hours)

CHUNKFORMER_RETURN_TIMESTAMPS=true
# Whether to return timestamps - Default: true

# Device Settings
CHUNKFORMER_DEVICE=cuda
# Options: cuda, cpu, auto

CHUNKFORMER_BATCH_SIZE=
# Leave empty for sequential/unbatched, or set batch size

CHUNKFORMER_CPU_FALLBACK=false

# ============================================================================
# LLM Settings - Text Enhancement
# ============================================================================
LLM_ENHANCE_MODEL=cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit
LLM_ENHANCE_GPU_MEMORY_UTILIZATION=0.95
LLM_ENHANCE_MAX_MODEL_LEN=32768
LLM_ENHANCE_TEMPERATURE=0.0
LLM_ENHANCE_TOP_P=0.9
LLM_ENHANCE_TOP_K=20
LLM_ENHANCE_MAX_TOKENS=4096

# ============================================================================
# LLM Settings - Summarization
# ============================================================================
LLM_SUM_MODEL=cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit
LLM_SUM_GPU_MEMORY_UTILIZATION=0.95
LLM_SUM_MAX_MODEL_LEN=32768
LLM_SUM_TEMPERATURE=0.7
LLM_SUM_TOP_P=0.9
LLM_SUM_TOP_K=20
LLM_SUM_MAX_TOKENS=4096

# ============================================================================
# File Paths
# ============================================================================
AUDIO_DIR=data/audio
MODELS_DIR=data/models
TEMPLATES_DIR=templates
CHAT_TEMPLATES_DIR=assets/chat-templates

# ============================================================================
# Worker Settings
# ============================================================================
WORKER_NAME=maie-worker
JOB_TIMEOUT=600
RESULT_TTL=86400

# ============================================================================
# Performance Tuning (Optional)
# ============================================================================
# CPU Performance (when running on CPU)
OMP_NUM_THREADS=4
# Adjust based on CPU cores: recommended = (CPU cores / 2)

# ============================================================================
# Notes
# ============================================================================
# 1. ChunkFormer parameters were corrected in V1.0:
#    - chunk_size: 64 frames (was incorrectly 16000 samples)
#    - total_batch_duration: 14400 seconds (was incorrectly documented as ms)
#
# 2. For Distil-Whisper models (e.g., distil-large-v3):
#    - Set WHISPER_CONDITION_ON_PREVIOUS_TEXT=false
#    - This is REQUIRED for proper accuracy
#
# 3. Language detection:
#    - Leave WHISPER_LANGUAGE empty for auto-detection
#    - Set to specific code (e.g., "en", "vi") to force language
#
# 4. GPU Memory:
#    - Adjust LLM_*_GPU_MEMORY_UTILIZATION if you get OOM errors
#    - Recommended range: 0.85-0.95
#
# 5. Security:
#    - ALWAYS change SECRET_API_KEY in production
#    - Use a strong random key (32+ characters)


