# MAIE (Modular Audio Intelligence Engine) Environment Configuration
#
# This template file contains base/shared environment variables for the MAIE system.
# This file is OPTIONAL and can be used for values shared across all environments.
#
# CONFIGURATION APPROACH:
# ======================
# MAIE uses environment-specific configuration files following Twelve-Factor App principles:
#
# 1. Base file (this file): .env - for shared values across all environments (optional)
# 2. Environment-specific: .env.development or .env.production - for environment overrides
#
# SETUP INSTRUCTIONS:
# ===================
# 1. Copy .env.development.template to .env.development (for development)
# 2. Copy .env.production.template to .env.production (for production)
# 3. Optionally copy this file to .env for shared values
# 4. Customize the values as needed
# 5. DO NOT commit .env, .env.development, or .env.production to version control
#
# CONFIGURATION PRECEDENCE (highest to lowest):
# =============================================
# 1. Environment variables (os.environ) - set in shell/container
# 2. .env.{environment} file (e.g., .env.development when ENVIRONMENT=development)
# 3. .env file (this file, if it exists) - base/shared values
# 4. Default values from model fields
#
# VARIABLE NAMING:
# ================
# All variables use APP_ prefix with __ (double underscore) for nested settings:
# - Top-level: APP_ENVIRONMENT, APP_DEBUG
# - Nested: APP_LOGGING__LOG_LEVEL, APP_API__SECRET_KEY
#
# Reference: TDD Section 3.1, 3.2, 3.3.1, 3.6, 3.7
# Security: Keep actual secrets secure and never commit to repository

# ============================================================================
# CORE CONFIGURATION
# ============================================================================

# Environment name (development/production)
# This determines which .env.{environment} file is loaded
APP_ENVIRONMENT=development

# Pipeline version for NFR-1
APP_PIPELINE_VERSION=1.0.0

# ============================================================================
# API CONFIGURATION
# ============================================================================

# API server host
APP_API__HOST=0.0.0.0

# API server port
APP_API__PORT=8000

# API authentication key (CHANGE IN PRODUCTION)
APP_API__SECRET_KEY=your_secret_api_key_here

# Maximum file upload size in MB
APP_API__MAX_FILE_SIZE_MB=500.0

# Default ASR backend to use when not specified in API requests
# Options: whisper, chunkformer
APP_API__DEFAULT_ASR_BACKEND=chunkformer

# ============================================================================
# REDIS CONFIGURATION
# ============================================================================

# Redis connection URL for task queues and data storage
APP_REDIS__URL=redis://localhost:6379/0

# Redis DB for task results
APP_REDIS__RESULTS_DB=1

# Maximum queue depth for backpressure management
# Reference: NFR-5
APP_REDIS__MAX_QUEUE_DEPTH=50

# ============================================================================
# RATE LIMITING
# ============================================================================

# Enable rate limiting for API endpoints
APP_RATE_LIMIT__ENABLED=true

# Rate limit specification: (unit, count)
# Unit: second, minute, hour, or day
# Count: maximum requests allowed within that time unit
# Example: ("minute", 60) allows 60 requests per minute
APP_RATE_LIMIT__LIMIT=("minute", 60)

# ============================================================================
# AUDIO PROCESSING (ASR - Whisper)
# ============================================================================

# Whisper model path
APP_ASR__WHISPER_MODEL_PATH=data/models/openai-whisper-large

# Whisper model variant
# Default: openai-large
# Supported: large-v3, turbo, distil-large-v3, custom CT2 models
APP_ASR__WHISPER_MODEL_VARIANT=openai-large

# Whisper beam size for decoding
APP_ASR__WHISPER_BEAM_SIZE=5

# Enable VAD filtering
APP_ASR__WHISPER_VAD_FILTER=false

# VAD minimum silence duration (ms)
APP_ASR__WHISPER_VAD_MIN_SILENCE_MS=500

# VAD speech padding (ms)
APP_ASR__WHISPER_VAD_SPEECH_PAD_MS=400

# Device for whisper computation (cuda, cpu, auto)
APP_ASR__WHISPER_DEVICE=cuda

# CTranslate2 compute type (float16, int8_float16, int8)
APP_ASR__WHISPER_COMPUTE_TYPE=float16

# Enable CPU fallback if CUDA fails
APP_ASR__WHISPER_CPU_FALLBACK=false

# Use previous text as context (set false for Distil-Whisper)
APP_ASR__WHISPER_CONDITION_ON_PREVIOUS_TEXT=false

# Force specific language code (e.g., 'en', 'vi') or leave empty for auto-detect
APP_ASR__WHISPER_LANGUAGE=

# CPU thread count for performance (recommended: CPU cores / 2)
APP_ASR__WHISPER_CPU_THREADS=

# Enable word-level timestamps
APP_ASR__WHISPER_WORD_TIMESTAMPS=true

# ============================================================================
# ASR HALLUCINATION FILTERING
# ============================================================================

# Enable filtering of common ASR hallucinations
APP_ASR__HALLUCINATION__ENABLED=true

# Maximum consecutive identical words before filtering
APP_ASR__HALLUCINATION__MAX_REPEATED_WORDS=3

# Maximum repeated phrases before filtering
APP_ASR__HALLUCINATION__MAX_REPEATED_PHRASES=3

# Minimum segment confidence threshold (0.0 to 1.0, optional)
APP_ASR__HALLUCINATION__MIN_SEGMENT_CONFIDENCE=

# Minimum word probability threshold (0.0 to 1.0, optional)
APP_ASR__HALLUCINATION__MIN_WORD_PROBABILITY=

# Path to JSON file containing ASR hallucination patterns
APP_ASR__HALLUCINATION__PATTERN_FILE=data/asr_hallucinations.json

# Minimum words per segment
APP_ASR__HALLUCINATION__MIN_SEGMENT_LENGTH=1

# Maximum words per segment (optional)
APP_ASR__HALLUCINATION__MAX_SEGMENT_LENGTH=

# ============================================================================
# CHUNKFORMER CONFIGURATION
# ============================================================================

# Chunkformer model path
APP_CHUNKFORMER__CHUNKFORMER_MODEL_PATH=data/models/chunkformer-rnnt-large-vie

# Chunkformer model variant
APP_CHUNKFORMER__CHUNKFORMER_MODEL_VARIANT=khanhld/chunkformer-rnnt-large-vie

# Chunk size in frames
APP_CHUNKFORMER__CHUNKFORMER_CHUNK_SIZE=64

# Left context size in frames
APP_CHUNKFORMER__CHUNKFORMER_LEFT_CONTEXT_SIZE=64

# Right context size in frames
APP_CHUNKFORMER__CHUNKFORMER_RIGHT_CONTEXT_SIZE=64

# Total batch duration in seconds
APP_CHUNKFORMER__CHUNKFORMER_TOTAL_BATCH_DURATION=7200

# Return timestamps
APP_CHUNKFORMER__CHUNKFORMER_RETURN_TIMESTAMPS=true

# Device (cuda, cpu)
APP_CHUNKFORMER__CHUNKFORMER_DEVICE=cuda

# Batch size (optional)
APP_CHUNKFORMER__CHUNKFORMER_BATCH_SIZE=

# Enable CPU fallback
APP_CHUNKFORMER__CHUNKFORMER_CPU_FALLBACK=false

# ============================================================================
# LLM CONFIGURATION
# ============================================================================

# LLM Backend Mode (local_vllm or vllm_server)
# - local_vllm: Load model in worker process (simpler setup)
# - vllm_server: Connect to external vLLM server (recommended for production)
APP_LLM_BACKEND=vllm_server

# ============================================================================
# LLM ENHANCEMENT CONFIGURATION (used when LLM_BACKEND=local_vllm)
# ============================================================================

# LLM model identifier for enhancement
APP_LLM_ENHANCE__MODEL=data/models/qwen3-4b-instruct-2507-awq

# GPU memory utilization (0.1 to 1.0)
APP_LLM_ENHANCE__GPU_MEMORY_UTILIZATION=0.9

# Maximum model context length
APP_LLM_ENHANCE__MAX_MODEL_LEN=32768

# LLM sampling temperature (0.0 to 2.0)
APP_LLM_ENHANCE__TEMPERATURE=0.5

# Nucleus sampling threshold (0.0 to 1.0, optional)
APP_LLM_ENHANCE__TOP_P=

# Top-k sampling (positive integer, optional)
APP_LLM_ENHANCE__TOP_K=

# Maximum tokens to generate (optional)
APP_LLM_ENHANCE__MAX_TOKENS=

# Use beam search
APP_LLM_ENHANCE__USE_BEAM_SEARCH=false

# Quantization (optional)
APP_LLM_ENHANCE__QUANTIZATION=

# Maximum concurrent sequences
APP_LLM_ENHANCE__MAX_NUM_SEQS=4

# Maximum batched tokens per scheduler step
APP_LLM_ENHANCE__MAX_NUM_BATCHED_TOKENS=8192

# Maximum partial prefills for chunked prefill (optional)
APP_LLM_ENHANCE__MAX_NUM_PARTIAL_PREFILLS=

# ============================================================================
# LLM SUMMARIZATION CONFIGURATION (used when LLM_BACKEND=local_vllm)
# ============================================================================

# LLM model identifier for summarization
APP_LLM_SUM__MODEL=cpatonn/Qwen3-4B-Instruct-2507-AWQ-4bit

# GPU memory utilization (0.1 to 1.0)
APP_LLM_SUM__GPU_MEMORY_UTILIZATION=0.9

# Maximum model context length
APP_LLM_SUM__MAX_MODEL_LEN=32768

# LLM sampling temperature (0.0 to 2.0)
APP_LLM_SUM__TEMPERATURE=0.5

# Nucleus sampling threshold (0.0 to 1.0, optional)
APP_LLM_SUM__TOP_P=

# Top-k sampling (positive integer, optional)
APP_LLM_SUM__TOP_K=

# Maximum tokens to generate (optional)
APP_LLM_SUM__MAX_TOKENS=

# Quantization (optional)
APP_LLM_SUM__QUANTIZATION=

# Maximum concurrent sequences (optional)
APP_LLM_SUM__MAX_NUM_SEQS=

# Maximum batched tokens per scheduler step (optional)
APP_LLM_SUM__MAX_NUM_BATCHED_TOKENS=

# Maximum partial prefills for chunked prefill (optional)
APP_LLM_SUM__MAX_NUM_PARTIAL_PREFILLS=

# Enable structured output generation
APP_LLM_SUM__STRUCTURED_OUTPUTS_ENABLED=false

# Structured outputs backend (xgrammar, guidance, outlines, lm-format-enforcer, auto)
APP_LLM_SUM__STRUCTURED_OUTPUTS_BACKEND=xgrammar

# ============================================================================
# LLM SERVER CONFIGURATION (used when LLM_BACKEND=vllm_server)
# ============================================================================

# Enhancement endpoint (required for vllm_server mode)
APP_LLM_SERVER__ENHANCE_BASE_URL=http://localhost:8001/v1

# Enhancement API key (optional)
APP_LLM_SERVER__ENHANCE_API_KEY=

# Enhancement model name (optional)
APP_LLM_SERVER__ENHANCE_MODEL_NAME=maie-enhance

# Summary endpoint (optional, defaults to enhancement endpoint)
APP_LLM_SERVER__SUMMARY_BASE_URL=

# Summary API key (optional)
APP_LLM_SERVER__SUMMARY_API_KEY=

# Summary model name (optional)
APP_LLM_SERVER__SUMMARY_MODEL_NAME=maie-enhance

# Request timeout for vLLM server (seconds)
APP_LLM_SERVER__REQUEST_TIMEOUT_SECONDS=300.0

# ============================================================================
# FILE PATHS
# ============================================================================

# Audio directory
APP_PATHS__AUDIO_DIR=data/audio

# Models directory
APP_PATHS__MODELS_DIR=data/models

# Templates directory
APP_PATHS__TEMPLATES_DIR=templates

# ============================================================================
# WORKER SETTINGS
# ============================================================================

# Worker identifier
APP_WORKER__WORKER_NAME=maie-worker

# Job timeout in seconds
APP_WORKER__JOB_TIMEOUT=600

# Result retention time in seconds (24h)
APP_WORKER__RESULT_TTL=86400

# Worker concurrency (number of concurrent jobs)
APP_WORKER__WORKER_CONCURRENCY=2

# Worker prefetch multiplier
APP_WORKER__WORKER_PREFETCH_MULTIPLIER=4

# Worker prefetch timeout in seconds
APP_WORKER__WORKER_PREFETCH_TIMEOUT=30

# ============================================================================
# CLEANUP & MAINTENANCE
# ============================================================================

# Audio cleanup interval in seconds (default: 3600 = 1 hour)
APP_CLEANUP__AUDIO_CLEANUP_INTERVAL=3600

# Log cleanup interval in seconds (default: 86400 = 24 hours)
APP_CLEANUP__LOG_CLEANUP_INTERVAL=86400

# Cache cleanup interval in seconds (default: 1800 = 30 minutes)
APP_CLEANUP__CACHE_CLEANUP_INTERVAL=1800

# Disk monitoring interval in seconds (default: 300 = 5 minutes)
APP_CLEANUP__DISK_MONITOR_INTERVAL=300

# Days to keep audio files (default: 7)
APP_CLEANUP__AUDIO_RETENTION_DAYS=7

# Days to keep log files (default: 7)
APP_CLEANUP__LOGS_RETENTION_DAYS=7

# Disk usage threshold percentage for alerts (default: 80)
APP_CLEANUP__DISK_THRESHOLD_PCT=80

# Enable automatic emergency cleanup on disk alerts (default: false)
APP_CLEANUP__EMERGENCY_CLEANUP=false

# Directory to monitor for disk usage (default: .)
APP_CLEANUP__CHECK_DIR=.

# ============================================================================
# DIARIZATION CONFIGURATION
# ============================================================================

# Enable speaker diarization
APP_DIARIZATION__ENABLED=false

# Model path (LOCAL PATH for fully offline operation)
APP_DIARIZATION__MODEL_PATH=data/models/pyannote-speaker-diarization-community-1

# Overlap threshold (0.0 to 1.0)
APP_DIARIZATION__OVERLAP_THRESHOLD=0.3

# Require CUDA for diarization
APP_DIARIZATION__REQUIRE_CUDA=false

# Embedding batch size (1 to 256)
APP_DIARIZATION__EMBEDDING_BATCH_SIZE=32

# Segmentation batch size (1 to 256)
APP_DIARIZATION__SEGMENTATION_BATCH_SIZE=32

# ============================================================================
# VAD (Voice Activity Detection) CONFIGURATION
# ============================================================================

# Enable Voice Activity Detection preprocessing
APP_VAD__ENABLED=true

# VAD backend (currently supports 'silero')
APP_VAD__BACKEND=silero

# Silero model path (optional, uses built-in loader if not provided)
APP_VAD__SILERO_MODEL_PATH=

# Silero threshold (0.0 to 1.0)
APP_VAD__SILERO_THRESHOLD=0.5

# Silero sampling rate (>= 8000)
APP_VAD__SILERO_SAMPLING_RATE=16000

# Minimum speech duration in milliseconds
APP_VAD__MIN_SPEECH_DURATION_MS=200

# Maximum speech duration in milliseconds
APP_VAD__MAX_SPEECH_DURATION_MS=60000

# Minimum silence duration in milliseconds
APP_VAD__MIN_SILENCE_DURATION_MS=500

# Window size for VAD in samples
APP_VAD__WINDOW_SIZE_SAMPLES=512

# Device (cuda, cpu)
APP_VAD__DEVICE=cuda

# ============================================================================
# FEATURE FLAGS
# ============================================================================

# Enable enhancement feature
APP_FEATURES__ENABLE_ENHANCEMENT=true

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================

# Log level (TRACE, DEBUG, INFO, SUCCESS, WARNING, ERROR, CRITICAL)
APP_LOGGING__LOG_LEVEL=INFO

# Log directory
APP_LOGGING__LOG_DIR=logs

# Log rotation size
APP_LOGGING__LOG_ROTATION=500 MB

# Log retention period
APP_LOGGING__LOG_RETENTION=30 days

# Serialize logs to console (JSON format)
APP_LOGGING__LOG_CONSOLE_SERIALIZE=false

# Serialize logs to file (JSON format)
APP_LOGGING__LOG_FILE_SERIALIZE=false

# Log compression format
APP_LOGGING__LOG_COMPRESSION=zip

# Enable loguru
APP_LOGGING__ENABLE_LOGURU=true

# Loguru diagnose mode
APP_LOGGING__LOGURU_DIAGNOSE=false

# Loguru backtrace
APP_LOGGING__LOGURU_BACKTRACE=true

# Loguru format (optional, uses default if not set)
APP_LOGGING__LOGURU_FORMAT=

# ============================================================================
# SECURITY & DEPLOYMENT NOTES
# ============================================================================
#
# 1. Do not commit your actual .env, .env.development, or .env.production files
# 2. Add "*.env" and ".env*" to your .gitignore (already done)
# 3. Use environment variables for production secrets (more secure)
# 4. Copy .env.development.template or .env.production.template as starting points
# 5. For production, prefer setting secrets via environment variables rather than files
#
# For environment-specific configurations, see:
# - .env.development.template (development settings)
# - .env.production.template (production settings)
